<?xml version="1.0" encoding="UTF-8"?>
<unit><root><base><html src="index.html"><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></html></base>Rpc_proxy</root><digest>c87d86d11399a261cd5030247bac0bd5</digest><import>Bigarray<digest>0643187a8b562858118690c393d01c2e</digest></import><import>Buffer<digest>a579f4a57e300ec755f84af883c1e51b</digest></import><import>CamlinternalFormatBasics<digest>ba1be62eb45abd435c75cb59cc46b922</digest></import><import>CamlinternalOO<digest>3d0b4eb4525ba4274c8885d7124f7bbc</digest></import><import>Complex<digest>d0dcc1da3c694cf9a7e924c7832d1528</digest></import><import>Equeue<digest>207ed21b384dd4f4ad91bce8a449f64c</digest></import><import>Hashtbl<digest>bb8e269d690301a1c4ff14a08e96e83e</digest></import><import>Int32<digest>f43b8a2972804b40e28b661b6fdf157a</digest></import><import>Int64<digest>3565b288ec68024088360805650448dd</digest></import><import>Lexing<digest>1be6a5484fb3cfb69d2c981438a7be62</digest></import><import>Netbuffer<digest>5f4225f2baeef1584e246c6ce1d32119</digest></import><import>Netchannels<digest>4e22bd1ba0cae4bd9bd1b6e69fc16ade</digest></import><import>Netexn<digest>845c6110cc81d957d9fc21c9e149d5fb</digest></import><import>Netlog<digest>2441459e41ceb77fc72d5714e0508958</digest></import><import>Netnumber<digest>dbd89ef42a315eaa84a5eeb21e4b6530</digest></import><import>Netsockaddr<digest>514bc9294568dc3facaf49b96623c337</digest></import><import>Netsys<digest>4a0ae4cc74f10e8667e3b629e6907079</digest></import><import>Netsys_mem<digest>f19ef786301a8cbcfbf1a4fffee992f8</digest></import><import>Netsys_posix<digest>f5b674538a41e016bb4f0e731d114657</digest></import><import>Netsys_signal<digest>703311022efc5162433d614b9bef35af</digest></import><import>Netsys_types<digest>67b26630eaf4decaa483864d4d00a3a2</digest></import><import>Netsys_win32<digest>02161f9169b76f686b05a51ed2d9357c</digest></import><import>Obj<digest>825c406ee2d12ed12fd12dc0e4de1d26</digest></import><import>Oo<digest>0977563fa9c23f2df2952b302bdba835</digest></import><import>Pervasives<digest>0d015a5a2136659b0de431be7f1545be</digest></import><import>Printf<digest>eb49a17645c5ea2dd298430a3c986186</digest></import><import>Rpc<digest>823bfd8fbbbc545fe6929685a57393b5</digest></import><import>Rpc_client<digest>56bf3aef28bd1d22eab29ca954a2cb3c</digest></import><import>Rpc_packer<digest>af7dd8ce3ff9294c27a0d09fd8424b4e</digest></import><import>Rpc_program<digest>2c8259e58584b03793c1d5b3eaa20436</digest></import><import>Rpc_transport<digest>6dccd6c9fbafeb7fe8a94a52c7614424</digest></import><import>Rpc_util<digest>fdef7f525a45859737db6e8e58372317</digest></import><import>Rtypes<digest>3a2195ceddcae11873afeb1784965176</digest></import><import>Set<digest>487197ccd2fea64d52f1cd917061caf2</digest></import><import>Stream<digest>55948988e71c3ee1749feb21ccec9fc9</digest></import><import>Sys<digest>0ce699458ce4430954d7e6a78874647c</digest></import><import>Unix<digest>30a4cc8e5f4f902ea609c91caec48af7</digest></import><import>Unixqueue<digest>5ec5f57d46c5b3e908696c79a59232d4</digest></import><import>Unixqueue_util<digest>f013da7473123405a6e2f9edc5dde5e1</digest></import><import>Uq_engines<digest>7eabb27fa86d9290a1fe2ef78547b48a</digest></import><import>Xdr<digest>d36f2013b5caa2d21a98f895fdddfd16</digest></import><import>Xdr_mstring<digest>2f3d19f64c1f9ca2f05941c7b5ea2c42</digest></import><source><file>rpc_proxy.mli</file><dir>/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc</dir><digest>bff1dfc56de5c5c8c654789a5294c33c</digest></source><doc>RPC proxies</doc><comment>The <code>Rpc_proxy</code> module provides an improved reliability layer on
top of <reference><element><root>Rpc_client</root></element></reference>. This layer especially features:<list><item>automatic connection management: TCP connections are started
and terminated as needed</item><item>multiple connections can be held in parallel to a remote
server to increase concurrency on the server</item><item>failover to other servers when the orignal servers time out</item><item>support for an initial ping at connection establishment time
to test the availability of the connection</item><item>retransmission of idempotent RPC calls</item></list>Proxies can only handle stream connections (TCP and Unix Domain).
Also, the remote endpoints must already be specified by socket
addresses. (No portmapper and other indirect lookup methods.)<newline/>The proxy functionality is implemented in two layers, the managed
clients, and the managed sets. The former layer can handle only
one TCP connection (with reconnect), whereas the latter is able to
manage a bunch of connections to the same service. Both layers
can profit from a reliability cache that knows which services had
errors in the past.<newline/>See below for a tutorial.<newline/>There is also a blog article explaining RPC proxies:
<reference><link>http://blog.camlcity.org/blog/ocamlnet3_ha.html</link>The next server,
please!</reference></comment><module><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module><type><signature><comment>The reliability cache stores information about the availability
of remote servers. The managed clients put information about
recent failures into the cache.<newline/>It is advantegeous to have only one cache per process, because
this maximizes the usefulness. The cache is thread-safe.<newline/>A server endpoint is disabled when too many errors occur in
sequence. For a disabled endpoint the functions <code>host_is_enabled</code>
and/or <code>sockaddr_is_enabled</code> return <code>false</code>. The endpoint is
automatically enabled again after some timeout; this is initially
<code>disable_timeout_min</code>, but is increased exponentially until
<code>disable_timeout_max</code> when further errors occur.<newline/>Independently of this machinery the functions <code>host_is_enabled</code>
and <code>sockaddr_is_enabled</code> may also return <code>false</code> when an
external availability checker says that the endpoint is down.
This information is not entered into the cache, and will also
not trigger the disable timeout. Instead, the hook function
getting the availability will be simply called again.</comment><type><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>rcache</type><doc>The cache</doc></type><type><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>rcache_policy</type><doc>How failures of individual ports are interpreted:<list><item><code>`Independent</code>: When a connection to a remote port repeatedly fails,
only this port is disabled</item><item><code>`Failing_port_disables_host p</code>: When a connection to the TCP
port <code>p</code> repeatedly fails, the whole remote host is disabled.
Other ports do not disable the host, but are treated as in
<code>`Independent</code>.</item><item><code>`Any_failing_port_disables_host</code>: When a connection to any TCP
port repeatedly fails, the whole remote host is disabled</item><item><code>`None</code>: Nothing is disabled</item></list>Note that the <code>rcache_availability</code> hook is not affected by the
policy; this hook is called anyway. The policy only determines
how the internal error counter is interpreted.</doc><poly_variant><fixed/><constructor>Independent<constant/></constructor><constructor>Failing_port_disables_host<path><resolved><identifier><type>int</type></identifier></resolved></path></constructor><constructor>Any_failing_port_disables_host<constant/></constructor><constructor>None<constant/></constructor></poly_variant></type><type><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>rcache_config</type><record><field><field><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>rcache_config</type>rcache_policy</field><doc>The policy, see above</doc><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>rcache_policy</type></identifier></resolved></path></field><field><field><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>rcache_config</type>rcache_disable_timeout_min</field><doc>For how long ports and hosts
are disabled</doc><path><resolved><identifier><type>float</type></identifier></resolved></path></field><field><field><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>rcache_config</type>rcache_disable_timeout_max</field><doc>For how long ports and hosts
are disabled at most</doc><path><resolved><identifier><type>float</type></identifier></resolved></path></field><field><field><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>rcache_config</type>rcache_threshold</field><doc>How many errors are required
for disabling a port</doc><path><resolved><identifier><type>int</type></identifier></resolved></path></field><field><field><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>rcache_config</type>rcache_availability</field><doc>External
availability checker. Called by <code>sockaddr_is_enabled</code> before
the result is calculated</doc><arrow><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>rcache</type></identifier></resolved></path><arrow><path><resolved><type><identifier><root><base><html src="../../../../ocaml.4.02.1+doc/otherlibs/unix/unix/index.html"><xml src="../../../../ocaml.4.02.1+doc/otherlibs/unix/unix/index.xml"><cmti name="Unix" src="/home/dsheets/.opam/doc/build/ocaml/otherlibs/unix/unix.cmti" digest="30a4cc8e5f4f902ea609c91caec48af7"/></xml></html></base>Unix</root></identifier>sockaddr</type></resolved></path><path><resolved><identifier><type>bool</type></identifier></resolved></path></arrow></arrow></field></record></type><value><value><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>create_rcache_config</value><doc>Create a config record. The optional arguments set the config
components with the same name. The arguments default to:<list><item><code>policy = `None</code></item><item><code>disable_timeout_min = 1.0</code></item><item><code>disable_timeout_max = 64.0</code></item><item><code>threshold = 1</code></item><item><code>availability = fun _ _ -&gt; true</code></item></list></doc><arrow><optional>policy</optional><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>rcache_policy</type></identifier></resolved></path><arrow><optional>disable_timeout_min</optional><path><resolved><identifier><type>float</type></identifier></resolved></path><arrow><optional>disable_timeout_max</optional><path><resolved><identifier><type>float</type></identifier></resolved></path><arrow><optional>threshold</optional><path><resolved><identifier><type>int</type></identifier></resolved></path><arrow><optional>availability</optional><arrow><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>rcache</type></identifier></resolved></path><arrow><path><resolved><type><identifier><root><base><html src="../../../../ocaml.4.02.1+doc/otherlibs/unix/unix/index.html"><xml src="../../../../ocaml.4.02.1+doc/otherlibs/unix/unix/index.xml"><cmti name="Unix" src="/home/dsheets/.opam/doc/build/ocaml/otherlibs/unix/unix.cmti" digest="30a4cc8e5f4f902ea609c91caec48af7"/></xml></html></base>Unix</root></identifier>sockaddr</type></resolved></path><path><resolved><identifier><type>bool</type></identifier></resolved></path></arrow></arrow><arrow><path><resolved><identifier><type>unit</type></identifier></resolved></path><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>rcache_config</type></identifier></resolved></path></arrow></arrow></arrow></arrow></arrow></arrow></value><value><value><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>create_rcache</value><doc>Creates a new cache object. The same cache can be used by several
managed clients, even by totally unrelated ones</doc><arrow><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>rcache_config</type></identifier></resolved></path><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>rcache</type></identifier></resolved></path></arrow></value><value><value><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>rcache_config</value><doc>Return the config</doc><arrow><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>rcache</type></identifier></resolved></path><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>rcache_config</type></identifier></resolved></path></arrow></value><value><value><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>global_rcache_config</value><doc>Returns the global config:<list><item><code>policy = `None</code></item><item><code>disable_timeout_min = 1.0</code></item><item><code>disable_timeout_max = 64.0</code></item><item><code>threshold = 1</code></item><item><code>availability = fun _ _ -&gt; true</code></item></list></doc><arrow><path><resolved><identifier><type>unit</type></identifier></resolved></path><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>rcache_config</type></identifier></resolved></path></arrow></value><value><value><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>set_global_rcache_config</value><doc>Sets the new global config. This is only possible as long as
neither <code>default_global_config</code> nor <code>global_rcache</code> have been called.</doc><arrow><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>rcache_config</type></identifier></resolved></path><path><resolved><identifier><type>unit</type></identifier></resolved></path></arrow></value><value><value><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>global_rcache</value><doc>The global cache. Initially, this cache has the default config.
It is possible to change the default config before using the
global cache for the first time.</doc><arrow><path><resolved><identifier><type>unit</type></identifier></resolved></path><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>rcache</type></identifier></resolved></path></arrow></value><value><value><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>derive_rcache</value><doc><code>derive_cache parent config</code>: Returns a new cache that shares the
error counters with <code>parent</code>. The interpretation of the counters,
however, may be differently configured in <code>config</code>.<newline/>Because it is advantageous to share the error information as much
as possible, the recommended way to create a new cache object is
to derive it from the global cache.<newline/>What <code>derive_rcache</code> actually does (and this is not yet
optimal): Any <code>incr</code> and <code>reset</code> of an error counter is also
forwarded to the parent cache. The tests whether hosts and ports
are enabled do an AND of the results for the cache and its parent
(i.e. both must be ok to enable). This allows some information
sharing, but only in vertical direction.</doc><arrow><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>rcache</type></identifier></resolved></path><arrow><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>rcache_config</type></identifier></resolved></path><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>rcache</type></identifier></resolved></path></arrow></arrow></value><value><value><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>incr_rcache_error_counter</value><doc>Increase the error counter for this sockaddr. If the threshold
is reached and there is a disable policy, the sockaddr will be disabled.<newline/>This function is to be called after an RPC call times out, or
runs into a socket error.</doc><arrow><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>rcache</type></identifier></resolved></path><arrow><path><resolved><type><identifier><root><base><html src="../../../../ocaml.4.02.1+doc/otherlibs/unix/unix/index.html"><xml src="../../../../ocaml.4.02.1+doc/otherlibs/unix/unix/index.xml"><cmti name="Unix" src="/home/dsheets/.opam/doc/build/ocaml/otherlibs/unix/unix.cmti" digest="30a4cc8e5f4f902ea609c91caec48af7"/></xml></html></base>Unix</root></identifier>sockaddr</type></resolved></path><path><resolved><identifier><type>unit</type></identifier></resolved></path></arrow></arrow></value><value><value><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>reset_rcache_error_counter</value><doc>Reset the error counter for this sockaddr. If disabled, the
sockaddr is set to enabled again.<newline/>This function is to be called when an RPC call is successful.</doc><arrow><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>rcache</type></identifier></resolved></path><arrow><path><resolved><type><identifier><root><base><html src="../../../../ocaml.4.02.1+doc/otherlibs/unix/unix/index.html"><xml src="../../../../ocaml.4.02.1+doc/otherlibs/unix/unix/index.xml"><cmti name="Unix" src="/home/dsheets/.opam/doc/build/ocaml/otherlibs/unix/unix.cmti" digest="30a4cc8e5f4f902ea609c91caec48af7"/></xml></html></base>Unix</root></identifier>sockaddr</type></resolved></path><path><resolved><identifier><type>unit</type></identifier></resolved></path></arrow></arrow></value><value><value><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>sockaddr_is_enabled</value><doc>Returns whether the sockaddr is enabled. This also calls the
<code>rcache_availability</code> hook.</doc><arrow><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>rcache</type></identifier></resolved></path><arrow><path><resolved><type><identifier><root><base><html src="../../../../ocaml.4.02.1+doc/otherlibs/unix/unix/index.html"><xml src="../../../../ocaml.4.02.1+doc/otherlibs/unix/unix/index.xml"><cmti name="Unix" src="/home/dsheets/.opam/doc/build/ocaml/otherlibs/unix/unix.cmti" digest="30a4cc8e5f4f902ea609c91caec48af7"/></xml></html></base>Unix</root></identifier>sockaddr</type></resolved></path><path><resolved><identifier><type>bool</type></identifier></resolved></path></arrow></arrow></value><value><value><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>host_is_enabled</value><doc>Returns whether the host is enabled</doc><arrow><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module>rcache</type></identifier></resolved></path><arrow><path><resolved><type><identifier><root><base><html src="../../../../ocaml.4.02.1+doc/otherlibs/unix/unix/index.html"><xml src="../../../../ocaml.4.02.1+doc/otherlibs/unix/unix/index.xml"><cmti name="Unix" src="/home/dsheets/.opam/doc/build/ocaml/otherlibs/unix/unix.cmti" digest="30a4cc8e5f4f902ea609c91caec48af7"/></xml></html></base>Unix</root></identifier>inet_addr</type></resolved></path><path><resolved><identifier><type>bool</type></identifier></resolved></path></arrow></arrow></value></signature></type></module><module><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module><type><signature><comment>Managed clients are <reference><element><root>Rpc_client</root></element></reference> clients with the ability to
reconnect in the case of errors.<newline/>Additional features:<list><item>they can also be disabled, either based on a time criterion or
a customizable hook. This encodes the assumption that failing
servers need some time to recover</item><item>unused connections are closed (driven by a timeout)</item><item>support for the initial ping after establishing the connection</item></list>Initial pings are useful to test whether the connection is
really working. Servers normally accept new TCP connections without
knowing whether there are resources for processing the connections
(i.e. whether there is a process or thread waiting for serving
it). Because of this, the client cannot assume that the TCP
connection is really up only because the <code>connect</code> system call
said the connection is there. The initial ping fixes this problem:
The null procedure is once called after the TCP connection has
been established. Only when this works the client believes the
connection is really up. It is required that <code>mclient_programs</code>
is configured with at least one program, and this program must
have a procedure number 0 of type <code>void -&gt; void</code>.<newline/>In multi-threaded programs, threads must not share managed clients.<newline/>Managed clients can be used together with ocamlrpcgen-generated
modules. Provided the generated module <code>M_clnt</code> contains the
client code for program <code>P</code> and version <code>V</code>, one can do<newline/><precode>         module MC = M_clnt.Make'P(Rpc_proxy.ManagedClient)</precode><newline/>and call RPCs <code>f</code> as in<newline/><precode>         let res = MC.V.f mc arg</precode><newline/>(if <code>mc</code> is the managed client, and <code>arg</code> the argument).</comment><type><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>mclient</type><doc>A managed client</doc></type><type><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>mclient_config</type><record><field><field><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>mclient_config</type>mclient_rcache</field><doc>The rcache</doc><path><resolved><type><identifier><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module></identifier>rcache</type></resolved></path></field><field><field><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>mclient_config</type>mclient_socket_config</field><doc>The socket configuration</doc><path><resolved><class_type><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/rpc/rpc_client/index.xml"><cmti name="Rpc_client" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_client.cmti" digest="56bf3aef28bd1d22eab29ca954a2cb3c"/></xml></base>Rpc_client</root></identifier>socket_config</class_type></resolved></path></field><field><field><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>mclient_config</type>mclient_idle_timeout</field><doc>After how many seconds unused connections are closed.
A negative value means never. 0 means immediately. A positive
value is a point in time in the future.</doc><path><resolved><identifier><type>float</type></identifier></resolved></path></field><field><field><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>mclient_config</type>mclient_programs</field><doc>The programs to bind</doc><path><resolved><identifier><type>list</type></identifier></resolved><path><resolved><type><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/rpc/rpc_program/index.xml"><cmti name="Rpc_program" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_program.cmti" digest="2c8259e58584b03793c1d5b3eaa20436"/></xml></base>Rpc_program</root></identifier>t</type></resolved></path></path></field><field><field><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>mclient_config</type>mclient_msg_timeout</field><doc>After how many seconds the reply must have been arrived.
A negative value means there is no timeout. 0 means immediately.
A positive
value is a point in time in the future.</doc><path><resolved><identifier><type>float</type></identifier></resolved></path></field><field><field><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>mclient_config</type>mclient_msg_timeout_is_fatal</field><doc>Whether a message timeout is to be considered as fatal error
(the client is shut down, and the error counter for the endpoint
is increased)</doc><path><resolved><identifier><type>bool</type></identifier></resolved></path></field><field><field><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>mclient_config</type>mclient_exception_handler</field><doc>Whether to call <reference><element><resolved><value><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/rpc/rpc_client/index.xml"><cmti name="Rpc_client" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_client.cmti" digest="56bf3aef28bd1d22eab29ca954a2cb3c"/></xml></base>Rpc_client</root></identifier>set_exception_handler</value></resolved></element></reference></doc><path><resolved><identifier><type>option</type></identifier></resolved><arrow><path><resolved><identifier><type>exn</type></identifier></resolved></path><path><resolved><identifier><type>unit</type></identifier></resolved></path></arrow></path></field><field><field><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>mclient_config</type>mclient_auth_methods</field><doc>Set these authentication methods in the client</doc><path><resolved><identifier><type>list</type></identifier></resolved><path><resolved><class_type><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/rpc/rpc_client/index.xml"><cmti name="Rpc_client" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_client.cmti" digest="56bf3aef28bd1d22eab29ca954a2cb3c"/></xml></base>Rpc_client</root></identifier>auth_method</class_type></resolved></path></path></field><field><field><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>mclient_config</type>mclient_user_name</field><doc>The user name for authentication, None = default user</doc><path><resolved><identifier><type>option</type></identifier></resolved><path><resolved><identifier><type>string</type></identifier></resolved></path></path></field><field><field><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>mclient_config</type>mclient_initial_ping</field><doc>Whether to call procedure 0 of the first program after
connection establishment (see comments above)</doc><path><resolved><identifier><type>bool</type></identifier></resolved></path></field><field><field><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>mclient_config</type>mclient_max_response_length</field><doc>The maximum response length. See
<reference><element><resolved><value><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/rpc/rpc_client/index.xml"><cmti name="Rpc_client" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_client.cmti" digest="56bf3aef28bd1d22eab29ca954a2cb3c"/></xml></base>Rpc_client</root></identifier>set_max_response_length</value></resolved></element></reference>.</doc><path><resolved><identifier><type>option</type></identifier></resolved><path><resolved><identifier><type>int</type></identifier></resolved></path></path></field><field><field><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>mclient_config</type>mclient_mstring_factories</field><doc>The factories to use for decoding managed strings</doc><path><resolved><identifier><type>option</type></identifier></resolved><path><resolved><type><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/netstring/xdr_mstring/index.xml"><cmti name="Xdr_mstring" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/netstring/xdr_mstring.cmti" digest="2f3d19f64c1f9ca2f05941c7b5ea2c42"/></xml></base>Xdr_mstring</root></identifier>named_mstring_factories</type></resolved></path></path></field></record></type><exception><exception><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>Service_unavailable</exception><doc>Procedure calls may end with this exception when the reliability
cache disables the service</doc></exception><value><value><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>create_mclient_config</value><arrow><optional>rcache</optional><path><resolved><type><identifier><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ReliabilityCache</module></identifier>rcache</type></resolved></path><arrow><optional>socket_config</optional><path><resolved><class_type><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/rpc/rpc_client/index.xml"><cmti name="Rpc_client" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_client.cmti" digest="56bf3aef28bd1d22eab29ca954a2cb3c"/></xml></base>Rpc_client</root></identifier>socket_config</class_type></resolved></path><arrow><optional>idle_timeout</optional><path><resolved><identifier><type>float</type></identifier></resolved></path><arrow><optional>programs</optional><path><resolved><identifier><type>list</type></identifier></resolved><path><resolved><type><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/rpc/rpc_program/index.xml"><cmti name="Rpc_program" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_program.cmti" digest="2c8259e58584b03793c1d5b3eaa20436"/></xml></base>Rpc_program</root></identifier>t</type></resolved></path></path><arrow><optional>msg_timeout</optional><path><resolved><identifier><type>float</type></identifier></resolved></path><arrow><optional>msg_timeout_is_fatal</optional><path><resolved><identifier><type>bool</type></identifier></resolved></path><arrow><optional>exception_handler</optional><arrow><path><resolved><identifier><type>exn</type></identifier></resolved></path><path><resolved><identifier><type>unit</type></identifier></resolved></path></arrow><arrow><optional>auth_methods</optional><path><resolved><identifier><type>list</type></identifier></resolved><path><resolved><class_type><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/rpc/rpc_client/index.xml"><cmti name="Rpc_client" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_client.cmti" digest="56bf3aef28bd1d22eab29ca954a2cb3c"/></xml></base>Rpc_client</root></identifier>auth_method</class_type></resolved></path></path><arrow><optional>user_name</optional><path><resolved><identifier><type>option</type></identifier></resolved><path><resolved><identifier><type>string</type></identifier></resolved></path></path><arrow><optional>initial_ping</optional><path><resolved><identifier><type>bool</type></identifier></resolved></path><arrow><optional>max_response_length</optional><path><resolved><identifier><type>int</type></identifier></resolved></path><arrow><optional>mstring_factories</optional><path><resolved><type><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/netstring/xdr_mstring/index.xml"><cmti name="Xdr_mstring" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/netstring/xdr_mstring.cmti" digest="2f3d19f64c1f9ca2f05941c7b5ea2c42"/></xml></base>Xdr_mstring</root></identifier>named_mstring_factories</type></resolved></path><arrow><path><resolved><identifier><type>unit</type></identifier></resolved></path><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>mclient_config</type></identifier></resolved></path></arrow></arrow></arrow></arrow></arrow></arrow></arrow></arrow></arrow></arrow></arrow></arrow></arrow></value><comment>Create a config record. The optional arguments set the config
components with the same name. The defaults are:<list><item><code>rcache</code>: Use the global reliability cache</item><item><code>socket_config</code>: <reference><element><resolved><value><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/rpc/rpc_client/index.xml"><cmti name="Rpc_client" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_client.cmti" digest="56bf3aef28bd1d22eab29ca954a2cb3c"/></xml></base>Rpc_client</root></identifier>default_socket_config</value></resolved></element></reference></item><item><code>programs</code>: The empty list. It is very advisable to fill this!</item><item><code>msg_timeout</code>: (-1), i.e. none</item><item><code>msg_timeout_is_fatal</code>: false</item><item><code>exception_handler</code>: None</item><item><code>auth_methods</code>: empty list</item><item><code>user_name</code>: None</item><item><code>initial_ping</code>: false</item><item><code>max_response_length</code>: None</item><item><code>mstring_factories</code>: None</item></list></comment><value><value><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>create_mclient</value><doc>Create a managed client for this config connecting to this
connector. Only <code>Internet</code> and <code>Unix</code> connectors are supported.</doc><arrow><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>mclient_config</type></identifier></resolved></path><arrow><path><resolved><type><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/rpc/rpc_client/index.xml"><cmti name="Rpc_client" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_client.cmti" digest="56bf3aef28bd1d22eab29ca954a2cb3c"/></xml></base>Rpc_client</root></identifier>connector</type></resolved></path><arrow><path><resolved><class_type><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/equeue/unixqueue/index.xml"><cmti name="Unixqueue" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/equeue/unixqueue.cmti" digest="5ec5f57d46c5b3e908696c79a59232d4"/></xml></base>Unixqueue</root></identifier>event_system</class_type></resolved></path><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>mclient</type></identifier></resolved></path></arrow></arrow></arrow></value><type><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>state</type><doc>The state:<list><item><code>`Down</code>: The initial state, and also reached after a socket
error, or after one of the shutdown functions is called.
Although <code>`Down</code>, there might still some cleanup to do.
When RPC functions are called, the client is automatically
revived.</item><item><code>`Connecting</code>: This state is used while the initial ping is
done. It does not reflect whether the client is really
TCP-connected. Without initial ping, this state cannot occur.</item><item><code>`Up s</code>: The client is (so far known) up and can be used.
<code>s</code> is the socket address of the local socket</item></list></doc><poly_variant><fixed/><constructor>Down<constant/></constructor><constructor>Connecting<constant/></constructor><constructor>Up<path><resolved><identifier><type>option</type></identifier></resolved><path><resolved><type><identifier><root><base><html src="../../../../ocaml.4.02.1+doc/otherlibs/unix/unix/index.html"><xml src="../../../../ocaml.4.02.1+doc/otherlibs/unix/unix/index.xml"><cmti name="Unix" src="/home/dsheets/.opam/doc/build/ocaml/otherlibs/unix/unix.cmti" digest="30a4cc8e5f4f902ea609c91caec48af7"/></xml></html></base>Unix</root></identifier>sockaddr</type></resolved></path></path></constructor></poly_variant></type><value><value><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>mclient_state</value><doc>Get the state</doc><arrow><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>mclient</type></identifier></resolved></path><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>state</type></identifier></resolved></path></arrow></value><value><value><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>mclient_serial</value><doc>Get the serial number of the connection. The serial number is
increased when the client is reconnected. If the client is down
the serial number of the next connection attempt is returned.</doc><arrow><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>mclient</type></identifier></resolved></path><path><resolved><identifier><type>int</type></identifier></resolved></path></arrow></value><value><value><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>pending_calls</value><doc>Returns the number of pending calls</doc><arrow><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>mclient</type></identifier></resolved></path><path><resolved><identifier><type>int</type></identifier></resolved></path></arrow></value><value><value><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>event_system</value><doc>Return the event system</doc><arrow><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>mclient</type></identifier></resolved></path><path><resolved><class_type><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/equeue/unixqueue/index.xml"><cmti name="Unixqueue" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/equeue/unixqueue.cmti" digest="5ec5f57d46c5b3e908696c79a59232d4"/></xml></base>Unixqueue</root></identifier>event_system</class_type></resolved></path></arrow></value><value><value><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>shut_down</value><arrow><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>mclient</type></identifier></resolved></path><path><resolved><identifier><type>unit</type></identifier></resolved></path></arrow></value><value><value><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>sync_shutdown</value><arrow><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>mclient</type></identifier></resolved></path><path><resolved><identifier><type>unit</type></identifier></resolved></path></arrow></value><value><value><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>trigger_shutdown</value><doc>Shut down the managed client. See the corresponding functions
<reference><element><resolved><value><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/rpc/rpc_client/index.xml"><cmti name="Rpc_client" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_client.cmti" digest="56bf3aef28bd1d22eab29ca954a2cb3c"/></xml></base>Rpc_client</root></identifier>shut_down</value></resolved></element></reference>, <reference><element><resolved><value><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/rpc/rpc_client/index.xml"><cmti name="Rpc_client" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_client.cmti" digest="56bf3aef28bd1d22eab29ca954a2cb3c"/></xml></base>Rpc_client</root></identifier>sync_shutdown</value></resolved></element></reference>, and
<reference><element><resolved><value><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/rpc/rpc_client/index.xml"><cmti name="Rpc_client" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_client.cmti" digest="56bf3aef28bd1d22eab29ca954a2cb3c"/></xml></base>Rpc_client</root></identifier>trigger_shutdown</value></resolved></element></reference></doc><arrow><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>mclient</type></identifier></resolved></path><arrow><arrow><path><resolved><identifier><type>unit</type></identifier></resolved></path><path><resolved><identifier><type>unit</type></identifier></resolved></path></arrow><path><resolved><identifier><type>unit</type></identifier></resolved></path></arrow></arrow></value><value><value><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>record_unavailability</value><doc>Increases the error counter in the reliability cache for this
connection. The only effect can be that the error counter
exceeds the <code>rcache_threshold</code> so that the server endpoint
is disabled for some time. However, this only affects new
connections, not existing ones.<newline/>For a stricter interpretation of errors see
<code>enforce_unavailability</code>.<newline/>The error counter is increased anyway when a socket error
happens, or an RPC call times out and <code>msg_timeout_is_fatal</code>
is set. This function can be used to also interpret other
misbehaviors as fatal errors.</doc><arrow><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>mclient</type></identifier></resolved></path><path><resolved><identifier><type>unit</type></identifier></resolved></path></arrow></value><value><value><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>enforce_unavailability</value><doc>Enforces that all pending procedure calls get the
<code>Service_unavailable</code> exception, and that the client is shut down.
The background is this: When the reliability cache discovers an
unavailable port or host, only the new call is stopped with this
exception, but older calls remain unaffected. This function
can be used to change the policy, and to stop even pending calls.<newline/>The difference to <code>trigger_shutdown</code> is that the pending RPC
calls get the exception <code>Service_unavailable</code> instead of
<reference><element><resolved><exception><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/rpc/rpc_client/index.xml"><cmti name="Rpc_client" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_client.cmti" digest="56bf3aef28bd1d22eab29ca954a2cb3c"/></xml></base>Rpc_client</root></identifier>Message_lost</exception></resolved></element></reference>, and that it is enforced that the
shutdown is recorded as fatal error in the reliability cache.</doc><arrow><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>mclient</type></identifier></resolved></path><path><resolved><identifier><type>unit</type></identifier></resolved></path></arrow></value><value><value><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>set_batch_call</value><doc>The next call is a batch call. See <reference><element><resolved><value><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/rpc/rpc_client/index.xml"><cmti name="Rpc_client" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_client.cmti" digest="56bf3aef28bd1d22eab29ca954a2cb3c"/></xml></base>Rpc_client</root></identifier>set_batch_call</value></resolved></element></reference></doc><arrow><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>mclient</type></identifier></resolved></path><path><resolved><identifier><type>unit</type></identifier></resolved></path></arrow></value><value><value><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>rpc_engine</value><doc>Call an RPC function in engine style:<newline/><precode>let e = rpc_engine mc f_rpc</precode><newline/>where <code>f_rpc</code> is one of the generated client functions (async
signature). The engine reaches <code>`Done r</code> when the result <code>r</code>
has arrived.<newline/>The engine is not abortable (shut the client down instead).</doc><arrow><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>mclient</type></identifier></resolved></path><arrow><arrow><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>mclient</type></identifier></resolved></path><arrow><var>a</var><arrow><arrow><arrow><path><resolved><identifier><type>unit</type></identifier></resolved></path><var>b</var></arrow><path><resolved><identifier><type>unit</type></identifier></resolved></path></arrow><path><resolved><identifier><type>unit</type></identifier></resolved></path></arrow></arrow></arrow><arrow><var>a</var><path><resolved><class_type><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/equeue/uq_engines/index.xml"><cmti name="Uq_engines" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/equeue/uq_engines.cmti" digest="7eabb27fa86d9290a1fe2ef78547b48a"/></xml></base>Uq_engines</root></identifier>engine</class_type></resolved><var>b</var></path></arrow></arrow></arrow></value><value><value><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>compare</value><doc><code>ManagedClient</code> can be used with <code>Set.Make</code> and <code>Map.Make</code></doc><arrow><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>mclient</type></identifier></resolved></path><arrow><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>mclient</type></identifier></resolved></path><path><resolved><identifier><type>int</type></identifier></resolved></path></arrow></arrow></value><include><with><resolved><module_type><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/rpc/rpc_client/index.xml"><cmti name="Rpc_client" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_client.cmti" digest="56bf3aef28bd1d22eab29ca954a2cb3c"/></xml></base>Rpc_client</root></identifier>USE_CLIENT</module_type></resolved><type><resolved><type><root/>t</type></resolved><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module>mclient</type></identifier></resolved></path></type></with></include></signature></type></module><module><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedSet</module><type><signature><comment>Manages a set of clients</comment><type><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedSet</module>mset</type><doc>a managed set</doc></type><type><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedSet</module>mset_policy</type><doc>Sets in which order managed clients are picked from the
<code>services</code> array passed to <code>create_mset</code>:<list><item><code>`Failover</code>: Picks an element from the first service
in <code>services</code> that is enabled and has free capacity.
That means that the first service is preferred until it is
maxed out or it fails, then the second service is preferred,
and so on.</item><item><code>`Balance_load</code>: Picks an element from the service in
<code>services</code> that is enabled and has the lowest load.</item></list></doc><poly_variant><fixed/><constructor>Failover<constant/></constructor><constructor>Balance_load<constant/></constructor></poly_variant></type><type><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedSet</module>mset_config</type><record><field><field><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedSet</module>mset_config</type>mset_mclient_config</field><doc>The mclient config</doc><path><resolved><type><identifier><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module></identifier>mclient_config</type></resolved></path></field><field><field><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedSet</module>mset_config</type>mset_policy</field><doc>The policy</doc><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedSet</module>mset_policy</type></identifier></resolved></path></field><field><field><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedSet</module>mset_config</type>mset_pending_calls_max</field><doc>When an mclient processes this number of calls at the same time,
it is considered as fully busy. (Value must by &gt; 0).</doc><path><resolved><identifier><type>int</type></identifier></resolved></path></field><field><field><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedSet</module>mset_config</type>mset_pending_calls_norm</field><doc>When an mclient processes less than this number of calls,
its load is considered as too light, and it is tried to put
more load on this client before opening another one</doc><path><resolved><identifier><type>int</type></identifier></resolved></path></field><field><field><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedSet</module>mset_config</type>mset_idempotent_max</field><doc>How often idempotent procedures may be tried to be called.
A negative value means infinite.</doc><path><resolved><identifier><type>int</type></identifier></resolved></path></field><field><field><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedSet</module>mset_config</type>mset_idempotent_wait</field><doc>Wait this number of seconds before trying again</doc><path><resolved><identifier><type>float</type></identifier></resolved></path></field></record></type><exception><exception><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedSet</module>Cluster_service_unavailable</exception><doc>Raised by <code>mset_pick</code> when no available endpoint can be found,
or all available endpoints have reached their maximum load.</doc></exception><value><value><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedSet</module>create_mset_config</value><doc>Create a config record. The optional arguments set the config
components with the same name. The defaults are:<list><item><code>mclient_config</code>: The default mclient config</item><item><code>policy</code>: <code>`Balance_load</code></item><item><code>pending_calls_max</code>: <code>max_int</code></item><item><code>pending_calls_norm</code>: 1</item><item><code>idempotent_max</code>: 3</item><item><code>idempotent_wait</code>: 5.0</item></list></doc><arrow><optional>mclient_config</optional><path><resolved><type><identifier><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module></identifier>mclient_config</type></resolved></path><arrow><optional>policy</optional><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedSet</module>mset_policy</type></identifier></resolved></path><arrow><optional>pending_calls_max</optional><path><resolved><identifier><type>int</type></identifier></resolved></path><arrow><optional>pending_calls_norm</optional><path><resolved><identifier><type>int</type></identifier></resolved></path><arrow><optional>idempotent_max</optional><path><resolved><identifier><type>int</type></identifier></resolved></path><arrow><optional>idempotent_wait</optional><path><resolved><identifier><type>float</type></identifier></resolved></path><arrow><path><resolved><identifier><type>unit</type></identifier></resolved></path><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedSet</module>mset_config</type></identifier></resolved></path></arrow></arrow></arrow></arrow></arrow></arrow></arrow></value><value><value><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedSet</module>create_mset</value><doc><code>create_mset config services</code>: The mset is created with <code>config</code>,
and the <code>services</code> array describes which ports are available,
and how often each port may be contacted (i.e. max number of
connections).</doc><arrow><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedSet</module>mset_config</type></identifier></resolved></path><arrow><path><resolved><identifier><type>array</type></identifier></resolved><tuple><path><resolved><type><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/rpc/rpc_client/index.xml"><cmti name="Rpc_client" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_client.cmti" digest="56bf3aef28bd1d22eab29ca954a2cb3c"/></xml></base>Rpc_client</root></identifier>connector</type></resolved></path><path><resolved><identifier><type>int</type></identifier></resolved></path></tuple></path><arrow><path><resolved><class_type><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/equeue/unixqueue/index.xml"><cmti name="Unixqueue" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/equeue/unixqueue.cmti" digest="5ec5f57d46c5b3e908696c79a59232d4"/></xml></base>Unixqueue</root></identifier>event_system</class_type></resolved></path><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedSet</module>mset</type></identifier></resolved></path></arrow></arrow></arrow></value><value><value><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedSet</module>mset_pick</value><doc>Pick an mclient for another call, or raise <code>Cluster_service_unavailable</code>.
The returned int is the index in the <code>mset_services</code> array.<newline/>If <code>from</code> is given, not all specified mclients qualify for this
call. In <code>from</code> one can pass a list of indexes pointing into
the <code>mset_services</code> array, and only from these mclients the
mclient is picked. For <code>`Failover</code> policies, the order given
in <code>from</code> is respected, and the mclients are checked from left
to right.</doc><arrow><optional>from</optional><path><resolved><identifier><type>list</type></identifier></resolved><path><resolved><identifier><type>int</type></identifier></resolved></path></path><arrow><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedSet</module>mset</type></identifier></resolved></path><tuple><path><resolved><type><identifier><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module></identifier>mclient</type></resolved></path><path><resolved><identifier><type>int</type></identifier></resolved></path></tuple></arrow></arrow></value><value><value><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedSet</module>mset_services</value><doc>Returns the service array</doc><arrow><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedSet</module>mset</type></identifier></resolved></path><path><resolved><identifier><type>array</type></identifier></resolved><tuple><path><resolved><type><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/rpc/rpc_client/index.xml"><cmti name="Rpc_client" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_client.cmti" digest="56bf3aef28bd1d22eab29ca954a2cb3c"/></xml></base>Rpc_client</root></identifier>connector</type></resolved></path><path><resolved><identifier><type>int</type></identifier></resolved></path></tuple></path></arrow></value><value><value><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedSet</module>mset_load</value><doc>Returns the number of pending calls per service</doc><arrow><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedSet</module>mset</type></identifier></resolved></path><path><resolved><identifier><type>array</type></identifier></resolved><path><resolved><identifier><type>int</type></identifier></resolved></path></path></arrow></value><value><value><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedSet</module>event_system</value><doc>Return the event system</doc><arrow><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedSet</module>mset</type></identifier></resolved></path><path><resolved><class_type><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/equeue/unixqueue/index.xml"><cmti name="Unixqueue" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/equeue/unixqueue.cmti" digest="5ec5f57d46c5b3e908696c79a59232d4"/></xml></base>Unixqueue</root></identifier>event_system</class_type></resolved></path></arrow></value><value><value><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedSet</module>shut_down</value><arrow><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedSet</module>mset</type></identifier></resolved></path><path><resolved><identifier><type>unit</type></identifier></resolved></path></arrow></value><value><value><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedSet</module>sync_shutdown</value><arrow><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedSet</module>mset</type></identifier></resolved></path><path><resolved><identifier><type>unit</type></identifier></resolved></path></arrow></value><value><value><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedSet</module>trigger_shutdown</value><doc>Shut down the managed set. See the corresponding functions
<reference><element><resolved><value><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/rpc/rpc_client/index.xml"><cmti name="Rpc_client" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_client.cmti" digest="56bf3aef28bd1d22eab29ca954a2cb3c"/></xml></base>Rpc_client</root></identifier>shut_down</value></resolved></element></reference>, <reference><element><resolved><value><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/rpc/rpc_client/index.xml"><cmti name="Rpc_client" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_client.cmti" digest="56bf3aef28bd1d22eab29ca954a2cb3c"/></xml></base>Rpc_client</root></identifier>sync_shutdown</value></resolved></element></reference>, and
<reference><element><resolved><value><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/rpc/rpc_client/index.xml"><cmti name="Rpc_client" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_client.cmti" digest="56bf3aef28bd1d22eab29ca954a2cb3c"/></xml></base>Rpc_client</root></identifier>trigger_shutdown</value></resolved></element></reference></doc><arrow><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedSet</module>mset</type></identifier></resolved></path><arrow><arrow><path><resolved><identifier><type>unit</type></identifier></resolved></path><path><resolved><identifier><type>unit</type></identifier></resolved></path></arrow><path><resolved><identifier><type>unit</type></identifier></resolved></path></arrow></arrow></value><value><value><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedSet</module>idempotent_async_call</value><doc><code>idempotent_async_call
           mset async_call arg emit</code>: Picks a new
<code>mclient</code> and calls <code>async_call mclient arg emit</code>.
If the call leads to a fatal error, a new <code>mclient</code>
is picked, and the call is repeated. In total, the call may be
tried <code>mset_idempotent_max</code> times. It is recommended to set
<code>rcache_threshold</code> to 1 when using this function because this
enforces that a different mclient is picked when the first one
fails.<newline/>Note that a timeout is not considered as a fatal error by default;
one has to enable that by setting <code>mclient_msg_timeout_is_fatal</code>.<newline/>Note that this form of function is compatible with the
generated <code>foo'async</code> functions of the language mapping.<newline/><code>from</code> has the same meaning as in <code>mset_pick</code>.</doc><arrow><optional>from</optional><path><resolved><identifier><type>list</type></identifier></resolved><path><resolved><identifier><type>int</type></identifier></resolved></path></path><arrow><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedSet</module>mset</type></identifier></resolved></path><arrow><arrow><path><resolved><type><identifier><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module></identifier>mclient</type></resolved></path><arrow><var>a</var><arrow><arrow><arrow><path><resolved><identifier><type>unit</type></identifier></resolved></path><var>b</var></arrow><path><resolved><identifier><type>unit</type></identifier></resolved></path></arrow><path><resolved><identifier><type>unit</type></identifier></resolved></path></arrow></arrow></arrow><arrow><var>a</var><arrow><arrow><arrow><path><resolved><identifier><type>unit</type></identifier></resolved></path><var>b</var></arrow><path><resolved><identifier><type>unit</type></identifier></resolved></path></arrow><path><resolved><identifier><type>unit</type></identifier></resolved></path></arrow></arrow></arrow></arrow></arrow></value><value><value><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedSet</module>idempotent_sync_call</value><doc>Synchronized version. Note that you have to pass an asynchronous
function as second argument. The result is synchronous, however.</doc><arrow><optional>from</optional><path><resolved><identifier><type>list</type></identifier></resolved><path><resolved><identifier><type>int</type></identifier></resolved></path></path><arrow><path><resolved><identifier><type><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedSet</module>mset</type></identifier></resolved></path><arrow><arrow><path><resolved><type><identifier><module><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>ManagedClient</module></identifier>mclient</type></resolved></path><arrow><var>a</var><arrow><arrow><arrow><path><resolved><identifier><type>unit</type></identifier></resolved></path><var>b</var></arrow><path><resolved><identifier><type>unit</type></identifier></resolved></path></arrow><path><resolved><identifier><type>unit</type></identifier></resolved></path></arrow></arrow></arrow><arrow><var>a</var><var>b</var></arrow></arrow></arrow></arrow></value></signature></type></module><comment><title level="1"><label><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>tut</label>The <code>Rpc_proxy</code> tutorial</title><newline/><title level="2"><label><root><base><xml src="index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root>mclient</label>Managed clients</title><newline/>A normal RPC client has a very limited lifecylce: It is created,
then a connection is made to an RPC service, messages are exchanged,
and finally the connection is terminated. After that the client
becomes unusable. In short, it is &quot;use once&quot; client.<newline/>In contrast to this, managed clients can be recycled. This is
especially useful for dealing with socket errors, and
connection terminations triggered by the RPC server.<newline/><bold>How to use managed clients:</bold> For a <italic>normal</italic> RPC client the
generator <code>ocamlrpcgen</code> creates all required glue code to easily
start RPC calls. For example, if a file <code>proto.x</code> is taken as input
for <code>ocamlrpcgen</code>, a piece of code doing a call could look like:<newline/><precode>      let client =
        Proto_clnt.PROG.VERS.create_client connector protocol
      let result =
        Proto_clnt.PROG.VERS.procedure client argument</precode><newline/>(Here, <code>PROG</code>, <code>VERS</code>, <code>procedure</code> are just placeholders for the
name of the program, the version identifier, and the procedure name.)<newline/>For RPC proxies, however, this is slightly more complicated. <code>ocamlrpcgen</code>
does not produce a managed client that is ready for use. Instead,
only a functor is provided that can take the
<reference><element><dot><resolved><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/rpc/rpc_proxy/index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root></identifier></resolved>ManagedlClient</dot></element></reference> module as input:<newline/><precode>      module M = Proto_clnt.Make'PROG(Rpc_proxy.ManagedClient)

      let esys =
        Unixqueue.create_unix_event_system()
      let mclient_config =
        Rpc_proxy.ManagedClient.create_mclient_config
          ~programs:[ Proto_clnt.PROG.VERS._program ]
          () in
      let mclient =
        Rpc_proxy.ManagedClient.create_mclient mclient_config connector esys
      let result =
        M.VERS.procedure mclient argument</precode><newline/>(The functor approach has been chosen, because it gives the
user more flexibility - it is possible to apply the functor
on other implementations of improved clients than
<reference><element><resolved><module><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/rpc/rpc_proxy/index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root></identifier>ManagedClient</module></resolved></element></reference>.)<newline/>Note that <code>esys</code> is always explicit, even in the case the
user only performs synchronous calls - the user should create
a new <code>esys</code> then, pass it to <code>mclient</code>, and ignore it otherwise.<newline/>Now, how does the recycling feature work? The managed client can be
in one of three states:<list><item><code>`Down</code>: The client is not connected. This is the initial state,
and the state after errors and terminated connections (no matter
whether triggered by the client or by the server)</item><item><code>`Connecting</code>: The client is busy (re)connecting (only used in
some cases)</item><item><code>`Up sockaddr</code>: The client is connected and has the socket address
<code>sockaddr</code></item></list>The state can be queried with <reference><element><resolved><value><module><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/rpc/rpc_proxy/index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root></identifier>ManagedClient</module>mclient_state</value></resolved></element></reference>.
When it is <code>`Down</code>, the next RPC call automatically starts the
reconnect to the service. When the connection is established, the
call is done, and the messages are exchanged that are representing
the call. After that, the state remains <code>`Up</code> after the call.<newline/>When the call stops because of an error, the error is reported to
the user in the normal way, and the client is shut down, i.e. after
an error the state is <code>`Down</code>. If the user decides to try the call
again, the client automatically reconnects following the outlined
rules. Note that managed clients never automatically retry calls
by themselves.<newline/>When the TCP connection is regularly shut down (either by the server
or by the client calling <reference><element><resolved><value><module><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/rpc/rpc_proxy/index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root></identifier>ManagedClient</module>shut_down</value></resolved></element></reference>), the
client state is changed to <code>`Down</code> at the next opportunity. Especially
a server-driven shutdown may first be detected when the next RPC call
is tried on the connection. This may or may not lead to an error
depending on the exact timing. In any way, the connection is finally
established again.<newline/>Of course, managed clients must be shut down after use, because
there is no other (automatic) way of recognizing that they are no
longer used. Call <reference><element><resolved><value><module><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/rpc/rpc_proxy/index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root></identifier>ManagedClient</module>shut_down</value></resolved></element></reference> for this.<newline/>Managed client also have a few more features that can be
enabled in <code>mclient_config</code>, especially:<list><item><bold>Initial ping</bold>: This means that the TCP connection is tested
before being used for user operations. The test is done by pinging
the service once (via the RPC null procedure). This is recommended
because some connectivity problems can first be detected when the
TCP connection is actually used.</item><item><bold>Idle timeout</bold>: The TCP connection is closed after it is
idle for some period of time. &quot;Idle&quot; means here that nothing is
being transmitted, and that no response from the server is expected.
The connection is closed at the first opportunity. The user should
be aware that this can only happen when the event loop for <code>esys</code>
is running. Especially for synchronous calls this is typically
not the case, so one would have to call <code>Unixqueue.run esys</code> now
and then to create opportunities for detecting the idle timeout.</item><item><bold>Reliability cache</bold>: The cache object counts errors, and can
disable certain service endpoints if they only produce errors.
This mostly makes sense when there are alternative endpoints,
i.e. in the context of a managed set (see below).</item></list></comment><comment><title level="2">Managed Sets</title><newline/>Managed sets are another layer on top of the managed
clients. These sets are able to manage several connections where
each is implemented as managed client. The connections can go to
the same server endpoint in order to parallelize RPCs at the
client side, or to several server endpoints that provide the same
service. The latter can be used for client-driven load balancing,
and for client-driven failover management of HA setups (HA = high
availability).<newline/>For creating a managed set, the code looks like<newline/><precode>      module M = Proto_clnt.Make'PROG(Rpc_proxy.ManagedClient)

      let esys =
        Unixqueue.create_unix_event_system()
      let mclient_config =
        Rpc_proxy.ManagedClient.create_mclient_config
          ~programs:[ Proto_clnt.PROG.VERS._program ]
          () in
      let mset_config =
        Rpc_proxy.ManagedSet.create_mset_config
          ~mclient_config
          () in
      let services =
        [| connector, n_connections; ... |] in
      let mset =
        Rpc_proxy.ManagedSet.create_mset 
          mset_config 
          services
          esys in
      let mclient, idx =
        Rpc_proxy.ManagedSet.mset_pick mset in
      let result =
        M.VERS.procedure mclient argument</precode><newline/>The managed clients are internally created by the set - one
only should pass in <code>mclient_config</code> so the set knows what kind of
client is preferred. For the simple application of maintaining
several connections to the same server, one would create the <code>mset</code>
with a one-element service array:<newline/><precode>       let services =
          [| connector, n_connections |]</precode><newline/>where <code>connector</code> describes the server port, and <code>n_connections</code> is
the maximum number of connections to create and maintain.
The <reference><element><resolved><value><module><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/rpc/rpc_proxy/index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root></identifier>ManagedSet</module>mset_pick</value></resolved></element></reference>
function creates internally up to <code>n_connections</code> managed clients,
and returns one of them. By default, it is not guaranteed that the
client is idle (meaning no previous call is pending) -
if the connections are all already busy, <code>mset_pick</code>
starts returning busy connections (but the least busy one first).<newline/>There are a number of options allowing to modify the default
behavior:<list><item>One can enforce that only idle clients are returned by <code>mset_pick</code>.
To do this, pass the argument <code>~mset_pending_calls_max:1</code> to
<reference><element><resolved><value><module><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/rpc/rpc_proxy/index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root></identifier>ManagedSet</module>create_mset_config</value></resolved></element></reference>. It can then happen
that no client is idle, and <code>mset_pick</code> will raise
<reference><element><resolved><exception><module><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/rpc/rpc_proxy/index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root></identifier>ManagedSet</module>Cluster_service_unavailable</exception></resolved></element></reference>.</item><item>If the <code>services</code> array has more than one element, they are
considered as equivalent service endpoints. <code>mset_pick</code> will
pick one of the endpoints. There are two policies controlling
the selection: With <code>~policy:`Balance_load</code> it is aimed at
sending roughly the same number of calls to all endpoints. With
<code>~policy:`Failover</code> the services are assigned precedences by the position
in the array (i.e. the first service is used as long as possible,
then the second service is used, etc.). The <code>policy</code> argument
is again to be passed to <reference><element><resolved><value><module><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/rpc/rpc_proxy/index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root></identifier>ManagedSet</module>create_mset_config</value></resolved></element></reference>.</item></list>Of course, managed sets must be shut down after use, because
there is no other (automatic) way of recognizing that they are no
longer used. Call <reference><element><resolved><value><module><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/rpc/rpc_proxy/index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root></identifier>ManagedSet</module>shut_down</value></resolved></element></reference> for this.<newline/><title level="2">Caching reliability data</title><newline/>The cache allows to disable certain hosts or ports when the error
counter reaches a limit. The service is disabled for a limited time span.
This is especially useful when there is an alternate port that can
jump in for the failing one, i.e. when the <code>services</code> array of a
managed set has two or more elements.<newline/>There is a single global cache object, but one can also create
specific cache objects. Generally, cache objects can be shared by
many managed clients and managed sets. The hope is that sharing
is useful because more data can be made available to users of
services. If you do not want to use the global cache object, you
can create your own, and configure it in <code>mclient_config</code>.<newline/>The global cache object is automatically used when nothing else
is specified. The global cache object is by default configured in
a way so it does not have any effect, though. So we have to
change this in order to enable the cache:<newline/><precode>     let rcache_config =
       Rpc_proxy.ReliabilityCache.create_rcache_config
        ~policy:`Independent
        ~threshold:3
        () in
     Rpc_proxy.ReliabilityCache.set_global_rcache_config rcache_config</precode><newline/>This means that 3 errors in sequence disable a service port. <code>`Independent</code>
means that each port is handled independently in this respect.<newline/>At the first time, the port is only disabled for one second. The
duration of the time span is increased by each additional error
until it reaches 64 seconds. These durations can be changed, of
course.<newline/>As the impact of changing the global cache object is sometimes
unpredictable, one can also create a private cache object
(<reference><element><resolved><value><module><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/rpc/rpc_proxy/index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root></identifier>ReliabilityCache</module>create_rcache</value></resolved></element></reference>). Another way is
to derive a semi-private object from the global one. This means
that the error counters are global, but the interpretation can
be set individually in each use. This would look like:<newline/><precode>    let rcache_config =
      Rpc_proxy.ReliabilityCache.create_rcache_config
        ~policy:`Independent
        ~threshold:3
        () in
    let rcache =
      Rpc_proxy.ReliabilityCache.derive_rcache
        (Rpc_proxy.ReliabilityCache.global_rcache())
        rcache_config in
    ...
    let mclient_config =
      Rpc_proxy.ManagedClient.create_mclient_config
        ...
        ~rcache
        ...
        ()</precode><newline/><title level="2">Idempotent calls</title><newline/>In the layer of managed sets there is some limited support for
automatically repeating failing idempotent RPC calls.<newline/>Instead of calling the RPC with<newline/><precode>      let mclient, idx =
        Rpc_proxy.ManagedSet.mset_pick mset in
      let result =
        M.VERS.procedure mclient argument</precode><newline/>one uses<newline/><precode>      let result =
        Rpc_proxy.ManagedSet.idempotent_sync_call
          mset
          M.VERS.procedure'async
          argument</precode><newline/>The effet is that <reference><element><resolved><value><module><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/rpc/rpc_proxy/index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root></identifier>ManagedSet</module>idempotent_sync_call</value></resolved></element></reference>
repeats automatically the call when an error occurs. It is
assumed that the call is idempotent so it can be repeated
without changing the meaning.<newline/>The call may be repeated several times. This is configured in
the managed set <code>mset</code> (parameter <code>mset_idempotent_max</code>).<newline/>Note that one has to pass the asynchronous version (suffix <code>'async</code>)
of the RPC wrapper even when doing a synchronous call.<newline/>Also see the documentation for
<reference><element><resolved><value><module><identifier><root><base><xml src="../../../../ocamlnet.3.7.7/src/rpc/rpc_proxy/index.xml"><cmti name="Rpc_proxy" src="/home/dsheets/.opam/doc/build/ocamlnet.3.7.7/src/rpc/rpc_proxy.cmti" digest="c87d86d11399a261cd5030247bac0bd5"/></xml></base>Rpc_proxy</root></identifier>ManagedSet</module>idempotent_async_call</value></resolved></element></reference>.</comment></unit>